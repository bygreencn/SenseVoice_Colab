{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bygreencn/SenseVoice_Colab/blob/main/SenseVoice_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZUg1t1WnYGg"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/FunAudioLLM/SenseVoice.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfOnupzNnm8G"
      },
      "outputs": [],
      "source": [
        "!cd SenseVoice && pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fQGFtYSx2YW"
      },
      "outputs": [],
      "source": [
        "!pip install -U funasr funasr-onnx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/funasr/FSMN-VAD"
      ],
      "metadata": {
        "id": "Fb1KRdHAPpwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fmwL4Lpn_Rw"
      },
      "outputs": [],
      "source": [
        "from funasr import AutoModel\n",
        "from funasr.utils.postprocess_utils import rich_transcription_postprocess\n",
        "\n",
        "model_dir = \"iic/SenseVoiceSmall\"\n",
        "\n",
        "\n",
        "model = AutoModel(\n",
        "    model=model_dir,\n",
        "    trust_remote_code=True,\n",
        "    remote_code=\"./model.py\",\n",
        "    vad_model=\"fsmn-vad\",\n",
        "    vad_kwargs={\"max_single_segment_time\": 30000},\n",
        "    device=None,\n",
        ")\n",
        "\n",
        "# en\n",
        "res = model.generate(\n",
        "    input=f\"{model.model_path}/example/en.mp3\",\n",
        "    cache={},\n",
        "    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n",
        "    use_itn=True,\n",
        "    batch_size_s=60,\n",
        "    merge_vad=True,\n",
        "    merge_length_s=15,\n",
        ")\n",
        "text = rich_transcription_postprocess(res[0][\"text\"])\n",
        "print(text)\n",
        "# zh\n",
        "res = model.generate(\n",
        "    input=f\"{model.model_path}/example/zh.mp3\",\n",
        "    cache={},\n",
        "    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n",
        "    use_itn=True,\n",
        "    batch_size_s=60,\n",
        "    merge_vad=True,  #\n",
        "    merge_length_s=15,\n",
        ")\n",
        "text = rich_transcription_postprocess(res[0][\"text\"])\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2hEkKBdNgIn"
      },
      "source": [
        "**No streaming**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdGkP0wtNe7J"
      },
      "outputs": [],
      "source": [
        "from funasr import AutoModel\n",
        "# paraformer-zh is a multi-functional asr model\n",
        "# use vad, punc, spk or not as you need\n",
        "model = AutoModel(model=\"paraformer-zh\", model_revision=\"v2.0.4\",\n",
        "                  vad_model=\"fsmn-vad\", vad_model_revision=\"v2.0.4\",\n",
        "                  punc_model=\"ct-punc-c\", punc_model_revision=\"v2.0.4\",\n",
        "                  # spk_model=\"cam++\", spk_model_revision=\"v2.0.2\",\n",
        "                  )\n",
        "res = model.generate(input=f\"{model.model_path}/example/asr_example.wav\",\n",
        "                     batch_size_s=300,\n",
        "                     hotword='魔搭')\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHdpDwvoNlfa"
      },
      "source": [
        "**Streaming**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aZRp8tPypVh"
      },
      "outputs": [],
      "source": [
        "from funasr import AutoModel\n",
        "\n",
        "chunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\n",
        "encoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\n",
        "decoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n",
        "\n",
        "model = AutoModel(model=\"paraformer-zh-streaming\", model_revision=\"v2.0.4\"#,\n",
        "                  #vad_model=\"fsmn-vad\", vad_model_revision=\"v2.0.4\",\n",
        "                  #punc_model=\"ct-punc-c\", punc_model_revision=\"v2.0.4\",\n",
        "                  # spk_model=\"cam++\", spk_model_revision=\"v2.0.2\",\n",
        "                  )\n",
        "\n",
        "import soundfile\n",
        "import os\n",
        "\n",
        "#wav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\n",
        "wav_file = \"./FSMN-VAD/example/vad_example.wav\"\n",
        "speech, sample_rate = soundfile.read(wav_file)\n",
        "chunk_stride = chunk_size[1] * 960 # 600ms\n",
        "\n",
        "cache = {}\n",
        "total_chunk_num = int(len((speech)-1)/chunk_stride+1)\n",
        "for i in range(total_chunk_num):\n",
        "    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n",
        "    is_final = i == total_chunk_num - 1\n",
        "    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n",
        "    print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cShX1D2i07L0"
      },
      "outputs": [],
      "source": [
        "print(model.model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vntHOAYFN7An"
      },
      "source": [
        "**Voice Activity Detection (Non-Streaming)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5B5fVaAN-Mz"
      },
      "outputs": [],
      "source": [
        "from funasr import AutoModel\n",
        "\n",
        "model = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\")\n",
        "#wav_file = f\"{model.model_path}/example/asr_example.wav\"\n",
        "wav_file = \"./FSMN-VAD/example/vad_example.wav\"\n",
        "res = model.generate(input=wav_file,data_type=(\"sound\"))\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V822CYctOAFt"
      },
      "source": [
        "**Voice Activity Detection (Streaming)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FUmpdZjFNMwA"
      },
      "outputs": [],
      "source": [
        "from funasr import AutoModel\n",
        "\n",
        "chunk_size = 200 # ms\n",
        "model = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\")\n",
        "\n",
        "import soundfile\n",
        "\n",
        "wav_file = f\"{model.model_path}/example/vad_example.wav\"\n",
        "speech, sample_rate = soundfile.read(wav_file)\n",
        "chunk_stride = int(chunk_size * sample_rate / 1000)\n",
        "\n",
        "cache = {}\n",
        "total_chunk_num = int(len((speech)-1)/chunk_stride+1)\n",
        "for i in range(total_chunk_num):\n",
        "    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n",
        "    is_final = i == total_chunk_num - 1\n",
        "    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n",
        "    if len(res[0][\"value\"]):\n",
        "        print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyxGY4TI4QOQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxaEwRa2imV5"
      },
      "source": [
        "Run with onnx offline test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeqJnDdFqqpC"
      },
      "outputs": [],
      "source": [
        "from funasr_onnx import Paraformer\n",
        "from pathlib import Path\n",
        "\n",
        "model_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n",
        "# model_dir = \"damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\n",
        "model = Paraformer(model_dir, batch_size=1, quantize=True, disable_update=True)\n",
        "# model = Paraformer(model_dir, batch_size=1, device_id=0)  # gpu\n",
        "\n",
        "# when using paraformer-large-vad-punc model, you can set plot_timestamp_to=\"./xx.png\" to get figure of alignment besides timestamps\n",
        "# model = Paraformer(model_dir, batch_size=1, plot_timestamp_to=\"test.png\")\n",
        "\n",
        "wav_path = [\"{}/.cache/modelscope/hub/{}/example/asr_example.wav\".format(Path.home(), model_dir)]\n",
        "\n",
        "result = model(wav_path)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -alt ~/.cache/modelscope/hub/damo/\n",
        "!ls -lat ~/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\n",
        "!ls -lat ~/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example"
      ],
      "metadata": {
        "id": "TjYFsYK00wrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FSMN-VAD**"
      ],
      "metadata": {
        "id": "0BWeeZLz2mXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from funasr_onnx import Fsmn_vad\n",
        "from pathlib import Path\n",
        "\n",
        "model_dir = \"damo/speech_fsmn_vad_zh-cn-16k-common-pytorch\"\n",
        "wav_path = '{}/.cache/modelscope/hub/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/example/vad_example.wav'.format(Path.home())\n",
        "\n",
        "model = Fsmn_vad(model_dir, quantize=True)\n",
        "\n",
        "result = model(wav_path)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "ELyWfZUN0J24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUz7GtjjsbXQ"
      },
      "outputs": [],
      "source": [
        "!ls -alt ~/.cache/modelscope/hub/damo/\n",
        "!ls -lat ~/.cache/modelscope/hub/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch\n",
        "!ls -lat ~/.cache/modelscope/hub/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/example"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FSMN-VAD-online**"
      ],
      "metadata": {
        "id": "iYI7Efpc2soV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from funasr_onnx import Fsmn_vad_online\n",
        "import soundfile\n",
        "from pathlib import Path\n",
        "\n",
        "model_dir = \"damo/speech_fsmn_vad_zh-cn-16k-common-pytorch\"\n",
        "wav_path = '{}/.cache/modelscope/hub/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/example/vad_example.wav'.format(Path.home())\n",
        "\n",
        "model = Fsmn_vad_online(model_dir)\n",
        "\n",
        "\n",
        "##online vad\n",
        "speech, sample_rate = soundfile.read(wav_path)\n",
        "speech_length = speech.shape[0]\n",
        "#\n",
        "sample_offset = 0\n",
        "step = 1600\n",
        "param_dict = {'in_cache': []}\n",
        "for sample_offset in range(0, speech_length, min(step, speech_length - sample_offset)):\n",
        "    if sample_offset + step >= speech_length - 1:\n",
        "        step = speech_length - sample_offset\n",
        "        is_final = True\n",
        "    else:\n",
        "        is_final = False\n",
        "    param_dict['is_final'] = is_final\n",
        "    segments_result = model(audio_in=speech[sample_offset: sample_offset + step],\n",
        "                            param_dict=param_dict)\n",
        "    if segments_result:\n",
        "        print(segments_result)"
      ],
      "metadata": {
        "id": "qoB-5i34P7oX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -alt ~/.cache/modelscope/hub/damo/\n",
        "!ls -lat ~/.cache/modelscope/hub/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch\n",
        "!ls -lat ~/.cache/modelscope/hub/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/example"
      ],
      "metadata": {
        "id": "9etU97QQ3OUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CT-Transformer**"
      ],
      "metadata": {
        "id": "LDKQXOU72zWs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIIOGXOHumGr"
      },
      "outputs": [],
      "source": [
        "from funasr_onnx import CT_Transformer\n",
        "\n",
        "model_dir = \"damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch\"\n",
        "model = CT_Transformer(model_dir, quantize=True)\n",
        "\n",
        "text_in=\"跨境河流是养育沿岸人民的生命之源长期以来为帮助下游地区防灾减灾中方技术人员在上游地区极为恶劣的自然条件下克服巨大困难甚至冒着生命危险向印方提供汛期水文资料处理紧急事件中方重视印方在跨境河流问题上的关切愿意进一步完善双方联合工作机制凡是中方能做的我们都会去做而且会做得更好我请印度朋友们放心中国在上游的任何开发利用都会经过科学规划和论证兼顾上下游的利益\"\n",
        "result = model(text_in)\n",
        "print(result[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -alt ~/.cache/modelscope/hub/damo/\n",
        "!ls -lat ~/.cache/modelscope/hub/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch\n",
        "!ls -lat ~/.cache/modelscope/hub/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/example"
      ],
      "metadata": {
        "id": "z7vE6hHi3Sn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CT-Transformer-online**"
      ],
      "metadata": {
        "id": "uINUiHOh2-Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from funasr_onnx import CT_Transformer_VadRealtime\n",
        "\n",
        "model_dir = \"damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727\"\n",
        "model = CT_Transformer_VadRealtime(model_dir, quantize=True)\n",
        "\n",
        "text_in  = \"跨境河流是养育沿岸|人民的生命之源长期以来为帮助下游地区防灾减灾中方技术人员|在上游地区极为恶劣的自然条件下克服巨大困难甚至冒着生命危险|向印方提供汛期水文资料处理紧急事件中方重视印方在跨境河流>问题上的关切|愿意进一步完善双方联合工作机制|凡是|中方能做的我们|都会去做而且会做得更好我请印度朋友们放心中国在上游的|任何开发利用都会经过科学|规划和论证兼顾上下游的利益\"\n",
        "\n",
        "vads = text_in.split(\"|\")\n",
        "rec_result_all=\"\"\n",
        "param_dict = {\"cache\": []}\n",
        "for vad in vads:\n",
        "    result = model(vad, param_dict=param_dict)\n",
        "    rec_result_all += result[0]\n",
        "\n",
        "print(rec_result_all)"
      ],
      "metadata": {
        "id": "xWArKOCx3AF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -alt ~/.cache/modelscope/hub/damo/\n",
        "!ls -lat ~/.cache/modelscope/hub/damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727\n",
        "!ls -lat ~/.cache/modelscope/hub/damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727/example"
      ],
      "metadata": {
        "id": "-uQWest63fp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYsdfUzImQP5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gd/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMcLLWFHmqmY"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/gd/MyDrive/SenseVoice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -alt /content/gd/MyDrive/SenseVoice"
      ],
      "metadata": {
        "id": "efzv40cb1zFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/gd/MyDrive/SenseVoice/*"
      ],
      "metadata": {
        "id": "vITFDTYo1s05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uCgCFa7maGW"
      },
      "outputs": [],
      "source": [
        "!cp -rf ~/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch /content/gd/MyDrive/SenseVoice/\n",
        "!cp -rf ~/.cache/modelscope/hub/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch /content/gd/MyDrive/SenseVoice/\n",
        "!cp -rf ~/.cache/modelscope/hub/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch /content/gd/MyDrive/SenseVoice/\n",
        "!cp -rf ~/.cache/modelscope/hub/damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727 /content/gd/MyDrive/SenseVoice/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGUgRb1W03shSM1ygHNALS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}